{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Katrin-Leberfinger/Hybrid-gender-debiased-music-recommendation/blob/main/004_interpretation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWHMmzSHaV4j",
        "outputId": "1ffd3144-203f-4eec-f2e1-f29740c8558c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.19.2-py3-none-any.whl (4.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 4.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.7.0-py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 5.3 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 56.0 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 81.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.5.18.1)\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.7.0 pyyaml-6.0 tokenizers-0.12.1 transformers-4.19.2\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L9D-5zOmltBw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from transformers import BertConfig, BertPreTrainedModel, BertModel, BertForSequenceClassification\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizer, BertModel, AdamW\n",
        "from torch import nn\n",
        "from transformers import BertModel\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.utils import shuffle\n",
        "import matplotlib.pyplot as plt\n",
        "import gc                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GzTFm7XWXmXv",
        "outputId": "69d42ea2-3293-486b-a9ee-3339545e8a9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWWKtwJxXmQC",
        "outputId": "1e08cddf-b1eb-4399-cb5a-a98bf5c93f76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Master Thesis/data\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/Master\\ Thesis/data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_col = 'lyrics_cleaned'\n",
        "model_dir = f\"./results/input_{text_col}_tags_userEmb_itemEmb_optim_rms_model_prajjwal1/bert-tiny/\""
      ],
      "metadata": {
        "id": "Y9L2mF_2tY_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vf103qDF0cNv"
      },
      "source": [
        "## Read Data: Lyrics "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lTSGXhgqhfXI"
      },
      "outputs": [],
      "source": [
        "dir = \"preprocessed_data/\"\n",
        "\n",
        "data_interaction = pd.read_csv(dir + \"data_interaction.csv\").drop(['Unnamed: 0'],axis=1)\n",
        "data_tracks_tags_lyrics = pd.read_csv(dir + \"data_tracks_tags_lyrics.csv\").drop(['Unnamed: 0'],axis=1)\n",
        "data_interaction_test = pd.read_csv(dir + \"data_interaction_test.csv\").drop(['Unnamed: 0'],axis=1)\n",
        "data_interaction_train = pd.read_csv(dir + \"data_interaction_train.csv\").drop(['Unnamed: 0'],axis=1)\n",
        "data_interaction_val = pd.read_csv(dir + \"data_interaction_val.csv\").drop(['Unnamed: 0'],axis=1)\n",
        "\n",
        "pos2item = {i:v for i, v in enumerate(data_tracks_tags_lyrics.track_id.sort_values().unique())}\n",
        "item2pos = {v:i for i, v in enumerate(data_tracks_tags_lyrics.track_id.sort_values().unique())}\n",
        "\n",
        "id2user = {i:v for i, v in enumerate(data_interaction.user_id.sort_values().unique())}\n",
        "user2id = {v:i for i, v in enumerate(data_interaction.user_id.sort_values().unique())}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_predictions = pd.read_csv(model_dir + \"predictions.csv\").drop(['Unnamed: 0'],axis=1)\n",
        "topn=100\n",
        "results = []\n",
        "results_df = pd.DataFrame(columns = ['user_id', 'track_id', 'count'])\n",
        "for user, user_prediction in data_predictions.groupby('user_id'):\n",
        "    results_df = results_df.append(user_prediction.sort_values('count', ascending=False).head(topn))\n",
        "data_interaction_new = results_df.merge(data_tracks_tags_lyrics, on = 'track_id')"
      ],
      "metadata": {
        "id": "U8oqjy9iqbJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_all_info = pd.read_csv(\"music/data_all_info.txt\", sep=\"\\t\").drop(['Unnamed: 0'],axis=1)\n",
        "data_all_info = data_all_info[['track_artist', 'gender_artist', 'track_id']]\n",
        "\n",
        "data_user = pd.read_csv(\"data_user.txt\", sep=\"\\t\").drop(['Unnamed: 0'],axis=1)\n",
        "data_user.columns = ['user_id', 'gender_user']\n",
        "\n",
        "data_classification = data_all_info\n",
        "replace_dict1 = {'m' : 0, 'f' : 1}\n",
        "replace_dict2 = {'male' : 0, 'female' : 1}\n",
        "data_classification['gender_artist'] = data_classification['gender_artist'].replace(replace_dict2)\n",
        "data_classification = data_classification.merge(data_tracks_tags_lyrics[['track_id', 'tags', 'abstract']], on = 'track_id')\n",
        "data_classification\n",
        "\n",
        "df_tmp = pd.merge(data_interaction_train, data_user, 'inner')\n",
        "df_all = pd.merge(df_tmp, data_all_info, on = 'track_id').drop_duplicates()\n",
        "\n",
        "replace_dict1 = {'m' : 0, 'f' : 1}\n",
        "replace_dict2 = {'male' : 0, 'female' : 1}\n",
        "df_all['gender_user'] = df_all['gender_user'].replace(replace_dict1)\n",
        "df_all['gender_artist'] = df_all['gender_artist'].replace(replace_dict2)\n",
        "\n",
        "df_train = df_all[df_all['count'] == 1]\n",
        "\n",
        "df_tmp = pd.merge(data_interaction_test, data_user, 'inner')\n",
        "df_all = pd.merge(df_tmp, data_all_info, on = 'track_id').drop_duplicates()\n",
        "\n",
        "df_all['gender_user'] = df_all['gender_user'].replace(replace_dict1)\n",
        "df_all['gender_artist'] = df_all['gender_artist'].replace(replace_dict2)\n",
        "\n",
        "df_test = df_all[df_all['count'] == 1]\n",
        "\n",
        "df_tmp = pd.merge(results_df, data_user, 'inner')\n",
        "df_all = pd.merge(df_tmp, data_all_info, on = 'track_id').drop_duplicates()\n",
        "\n",
        "df_all['gender_user'] = df_all['gender_user'].replace(replace_dict1)\n",
        "df_all['gender_artist'] = df_all['gender_artist'].replace(replace_dict2)\n",
        "\n",
        "df_rec = df_all"
      ],
      "metadata": {
        "id": "oMJmegOrrNOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "class LFM2bDataset(Dataset):\n",
        "    def __init__(self, data_all,tokenizer,max_length, text_col, item2pos, user2id):\n",
        "        super(LFM2bDataset, self).__init__()\n",
        "        self.data_all = data_all\n",
        "        self.max_length = max_length\n",
        "        self.tokenizer = tokenizer\n",
        "        self.text_col = text_col\n",
        "        self.item2pos = item2pos\n",
        "        self.user2id = user2id\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.data_all)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        \n",
        "        text1 = self.data_all.iloc[index][self.text_col]\n",
        "        user_id = self.user2id[ self.data_all.iloc[index]['user_id']]\n",
        "        track_id = self.item2pos[ self.data_all.iloc[index]['track_id']]\n",
        "        \n",
        "        # inputs = tokenizer.encode_plus(\n",
        "        #     text1, \n",
        "        #     None,\n",
        "        #     add_special_tokens=True,\n",
        "        #     max_length = self.max_length,\n",
        "        #     pad_to_max_length=True   \n",
        "        #)\n",
        "\n",
        "        inputs = tokenizer.encode_plus(\n",
        "                text1, \n",
        "                add_special_tokens=True,\n",
        "                padding='max_length',\n",
        "                max_length = self.max_length,\n",
        "                return_tensors='pt',\n",
        "                truncation=True,\n",
        "                return_attention_mask=True\n",
        "                )\n",
        "        ids = inputs[\"input_ids\"]\n",
        "        mask = inputs[\"attention_mask\"]\n",
        "\n",
        "        return {\n",
        "            'input_ids_lyrics': ids.flatten(),\n",
        "            'attention_mask_lyrics': mask.flatten(),\n",
        "            'user_id': torch.tensor(user_id, dtype=torch.long),\n",
        "             'track_id': torch.tensor(track_id, dtype=torch.long),\n",
        "            'target': torch.tensor(self.data_all.iloc[index]['count'], dtype=torch.float)\n",
        "            }\n"
      ],
      "metadata": {
        "id": "GBsb7IbZqLF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "class LFM2bDatasetMulitpleText(Dataset):\n",
        "    def __init__(self, data_all,tokenizer,max_length, item2pos, user2id):\n",
        "        super(LFM2bDatasetMulitpleText, self).__init__()\n",
        "        self.data_all = data_all\n",
        "        self.max_length = max_length\n",
        "        self.tokenizer = tokenizer\n",
        "        self.item2pos = item2pos\n",
        "        self.user2id = user2id\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.data_all)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        \n",
        "        text1 = self.data_all.iloc[index]['lyrics_cleaned']\n",
        "        text2 = self.data_all.iloc[index]['tags']\n",
        "        text3 = self.data_all.iloc[index]['abstract']\n",
        "        user_id = self.user2id[ self.data_all.iloc[index]['user_id']]\n",
        "        track_id = self.item2pos[ self.data_all.iloc[index]['track_id']]\n",
        "        \n",
        "        inputs1 = tokenizer.encode_plus(\n",
        "            text1, \n",
        "            add_special_tokens=True,\n",
        "            padding='max_length',\n",
        "            max_length = self.max_length,\n",
        "            return_tensors='pt',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True\n",
        "            \n",
        "        )\n",
        "        ids1 = inputs1[\"input_ids\"]\n",
        "        mask1 = inputs1[\"attention_mask\"]\n",
        "\n",
        "        inputs2 = tokenizer.encode_plus(\n",
        "            text2, \n",
        "            add_special_tokens=True,\n",
        "            padding='max_length',\n",
        "            max_length = self.max_length,\n",
        "            return_tensors='pt',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True\n",
        "            \n",
        "        )\n",
        "        ids2 = inputs2[\"input_ids\"]\n",
        "        mask2 = inputs2[\"attention_mask\"]\n",
        "\n",
        "        inputs3 = tokenizer.encode_plus(\n",
        "            text3, \n",
        "            add_special_tokens=True,\n",
        "            padding='max_length',\n",
        "            max_length = self.max_length,\n",
        "            return_tensors='pt',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True\n",
        "            \n",
        "        )\n",
        "        ids3 = inputs1[\"input_ids\"]\n",
        "        mask3 = inputs1[\"attention_mask\"]\n",
        "\n",
        "        return {\n",
        "            'input_ids_lyrics': torch.tensor(ids1, dtype=torch.long),\n",
        "            'attention_mask_lyrics': torch.tensor(mask1, dtype=torch.long),\n",
        "            'input_ids_tags': torch.tensor(ids2, dtype=torch.long),\n",
        "            'attention_mask_tags': torch.tensor(mask2, dtype=torch.long),\n",
        "            'input_ids_abstract': torch.tensor(ids3, dtype=torch.long),\n",
        "            'attention_mask_abstract': torch.tensor(mask3, dtype=torch.long),\n",
        "            'user_id': torch.tensor(user_id, dtype=torch.long),\n",
        "            'track_id': torch.tensor(track_id, dtype=torch.long),\n",
        "            'target': torch.tensor(self.data_all.iloc[index]['count'], dtype=torch.float)\n",
        "            }\n"
      ],
      "metadata": {
        "id": "mNcNnRGZqMvg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbf_Xg4sE_gu"
      },
      "source": [
        "## **BERT Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_Vn354IdSzX"
      },
      "outputs": [],
      "source": [
        "class AMARBase(nn.Module):\n",
        "  \"\"\"Model with LT tables for user and items.\"\"\"\n",
        "  def __init__(self, hidden_dense_layer_size, item_embeddings_size, user_embeddings_size, num_users, num_items):\n",
        "      super(AMARBase, self).__init__()\n",
        "\n",
        "      self.hidden_dense_layer_size = hidden_dense_layer_size\n",
        "      self.item_embeddings_size = item_embeddings_size\n",
        "      self.user_embeddings_size = user_embeddings_size\n",
        "      self.num_items = num_items\n",
        "      self.num_users = num_users\n",
        "\n",
        "      self.model1_layer1 = nn.Embedding(self.num_items, self.item_embeddings_size, max_norm = 1.)\n",
        "      nn.init.uniform_(self.model1_layer1.weight, a=-0.05, b=0.05)\n",
        "\n",
        "      self.model2_layer1 = nn.Embedding(self.num_users, self.user_embeddings_size, max_norm = 1.)\n",
        "      nn.init.uniform_(self.model2_layer1.weight, a=-0.05, b=0.05)\n",
        "\n",
        "      self.linear1 = nn.Linear(self.hidden_dense_layer_size, 128)\n",
        "\n",
        "      self.relu = nn.ReLU()\n",
        "      \n",
        "      self.linear2 = nn.Linear(128, 1)\n",
        "\n",
        "      self.drop =  nn.Dropout(p=0.2)\n",
        "      self.sigmoid = nn.Sigmoid()\n",
        "      \n",
        "  def forward(self, x):\n",
        "      y1 = self.model1_layer1(x[0])\n",
        "      \n",
        "      y2 = self.model2_layer1(x[2])\n",
        "\n",
        "      y = torch.cat((y1, y2), 1) \n",
        "      y = self.linear1(y)\n",
        "      y = self.relu(y)\n",
        "      y = self.linear2(y)\n",
        "      return self.sigmoid(y)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoConfig, AutoModel, AutoTokenizer\n",
        "MODEL_NAME = 'distilbert-base-uncased'\n",
        "config = AutoConfig.from_pretrained(MODEL_NAME, num_labels = 2)\n",
        "\n",
        "hidden_dense_layer_size=778\n",
        "\n",
        "MODEL_NAME = 'prajjwal1/bert-tiny'\n",
        "#MODEL_NAME = 'bert-base-uncased'"
      ],
      "metadata": {
        "id": "Im1yGgNA0RZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHWjCxuHb6cX"
      },
      "outputs": [],
      "source": [
        "class AMARBertEmb(nn.Module):\n",
        "    def __init__(self, hidden_dense_layer_size, item_embeddings_size, user_embeddings_size, item_text_embeddings_size, num_users, num_items, hidden_layer=32):\n",
        "        super(AMARBert, self).__init__()\n",
        "\n",
        "        self.hidden_dense_layer_size = hidden_dense_layer_size\n",
        "        self.item_embeddings_size = item_embeddings_size\n",
        "        self.user_embeddings_size = user_embeddings_size\n",
        "        self.item_text_embeddings_size = item_text_embeddings_size\n",
        "        self.num_items = num_items\n",
        "        self.num_users = num_users\n",
        "        \n",
        "        self.model1_layer2 = BertModel.from_pretrained(MODEL_NAME)\n",
        "        self.model1_layer3 = nn.Dropout(p=0.3)\n",
        "\n",
        "        self.model2_layer1 = nn.Embedding(self.num_users, self.user_embeddings_size)\n",
        "        nn.init.uniform_(self.model2_layer1.weight, a=0.0, b=0.05)\n",
        "\n",
        "        self.model3_layer1 = nn.Embedding(self.num_items, self.item_embeddings_size)\n",
        "        nn.init.uniform_(self.model3_layer1.weight, a=0.0, b=0.05)\n",
        "        \n",
        "        self.linear1 = nn.Linear(self.hidden_dense_layer_size, self.hidden_dense_layer_size)\n",
        "        nn.init.xavier_uniform(self.linear1.weight)\n",
        "        self.linear1.bias.data.fill_(0.01)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(hidden_dense_layer_size, hidden_layer)\n",
        "        nn.init.xavier_uniform(self.linear2.weight)\n",
        "        self.linear2.bias.data.fill_(0.01)\n",
        "\n",
        "        self.linear3 = nn.Linear(hidden_layer, 1)\n",
        "        nn.init.xavier_uniform(self.linear3.weight)\n",
        "        self.linear3.bias.data.fill_(0.01)\n",
        "\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        output = self.model1_layer2(x[1], attention_mask = x[3])\n",
        "        y1 = output[0]\n",
        "        #y1 = y1.mean(axis=2) # alterantive to CLS token, combine with y1[:,0,:] possible\n",
        "        y1 = y1[:,0,:]\n",
        "        # \"Since BERT is transformer based contextual model, the idea is [CLS] token would have captured the entire context and would be sufficient for simple downstream tasks such as classification.\"\n",
        "        y1 = self.model1_layer3(y1)\n",
        "        \n",
        "        y2 = self.model2_layer1(x[2])\n",
        "\n",
        "        y3 = self.model3_layer1(x[0])\n",
        "\n",
        "        y = torch.cat([y1, y2, y3], 1)\n",
        "        y = self.linear1(y)\n",
        "        y = self.relu(y)\n",
        "        y = self.linear2(y)\n",
        "        y = self.linear3(y)\n",
        "        return self.sigmoid(y)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AMARBertMultiple(nn.Module):\n",
        "    def __init__(self, hidden_dense_layer_size, item_embeddings_size, user_embeddings_size, item_text_embeddings_size, num_users, num_items, hidden_layer=32):\n",
        "        super(AMARBertMultiple, self).__init__()\n",
        "\n",
        "        self.hidden_dense_layer_size = hidden_dense_layer_size\n",
        "        self.item_embeddings_size = item_embeddings_size\n",
        "        self.user_embeddings_size = user_embeddings_size\n",
        "        self.item_text_embeddings_size = item_text_embeddings_size\n",
        "        self.num_items = num_items\n",
        "        self.num_users = num_users\n",
        "        \n",
        "        self.model1_layer2 = BertModel.from_pretrained('prajjwal1/bert-tiny')\n",
        "        self.model1_layer3 = nn.Dropout(p=0.2)\n",
        "\n",
        "        self.model4_layer2 = BertModel.from_pretrained('prajjwal1/bert-tiny')\n",
        "        self.model4_layer3 = nn.Dropout(p=0.2)\n",
        "\n",
        "        self.model2_layer1 = nn.Embedding(self.num_users, self.user_embeddings_size, max_norm = 1.)\n",
        "        nn.init.uniform_(self.model2_layer1.weight, a=0.0, b=0.05)\n",
        "\n",
        "        self.model3_layer1 = nn.Embedding(self.num_items, self.item_embeddings_size, max_norm = 1.)\n",
        "        nn.init.uniform_(self.model3_layer1.weight, a=0.0, b=0.05)\n",
        "        \n",
        "        self.linear1 = nn.Linear(self.hidden_dense_layer_size, self.hidden_dense_layer_size)\n",
        "        nn.init.xavier_uniform(self.linear1.weight)\n",
        "        self.linear1.bias.data.fill_(0.01)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(hidden_dense_layer_size, hidden_layer)\n",
        "        nn.init.xavier_uniform(self.linear2.weight)\n",
        "        self.linear2.bias.data.fill_(0.01)\n",
        "\n",
        "        self.linear3 = nn.Linear(hidden_layer, 1)\n",
        "        nn.init.xavier_uniform(self.linear3.weight)\n",
        "        self.linear3.bias.data.fill_(0.01)\n",
        "\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        output = self.model1_layer2(x[1], attention_mask = x[3])\n",
        "        y1 = output[0]\n",
        "        #y1 = y1.mean(axis=2) # alterantive to CLS token, combine with y1[:,0,:] possible\n",
        "        y1 = y1[:,0,:]\n",
        "        # \"Since BERT is transformer based contextual model, the idea is [CLS] token would have captured the entire context and would be sufficient for simple downstream tasks such as classification.\"\n",
        "        y1 = self.model1_layer3(y1)\n",
        "\n",
        "        output = self.model1_layer2(x[4], attention_mask = x[5])\n",
        "        y4 = output[0]\n",
        "        #y1 = y1.mean(axis=2) # alterantive to CLS token, combine with y1[:,0,:] possible\n",
        "        y4 = y4[:,0,:]\n",
        "        # \"Since BERT is transformer based contextual model, the idea is [CLS] token would have captured the entire context and would be sufficient for simple downstream tasks such as classification.\"\n",
        "        y4 = self.model4_layer3(y4)\n",
        "        \n",
        "        y2 = self.model2_layer1(x[2])\n",
        "\n",
        "        y3 = self.model3_layer1(x[0])\n",
        "\n",
        "        y = torch.cat([y1, y2, y3, y4], 1)\n",
        "        \n",
        "        y = self.linear1(y)\n",
        "        y = self.relu(y)\n",
        "        y = self.linear2(y)\n",
        "        y = self.linear3(y)\n",
        "        \n",
        "        return self.sigmoid(y)\n"
      ],
      "metadata": {
        "id": "63hOMXmqD85X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AMARBert(nn.Module):\n",
        "    def __init__(self, hidden_dense_layer_size, item_embeddings_size, num_users, hidden_layer=32):\n",
        "        super(AMARBert, self).__init__()\n",
        "        self.model1_layer2 = BertModel.from_pretrained(MODEL_NAME)\n",
        "        self.model1_layer3 = nn.Dropout(p=0.3)\n",
        "\n",
        "        self.model2_layer1 = nn.Embedding(num_users, user_embeddings_size)\n",
        "        \n",
        "        self.linear = nn.Linear(hidden_dense_layer_size, hidden_dense_layer_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(hidden_dense_layer_size, hidden_layer)\n",
        "        self.linear3 = nn.Linear(hidden_layer, 1)\n",
        "        \n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        output = self.model1_layer2(x[1], attention_mask = x[3])\n",
        "        y1 = output[0]\n",
        "        #y1 = y1.mean(axis=2) # alterantive to CLS token, combine with y1[:,0,:] possible\n",
        "        y1 = y1[:, 0, :]\n",
        "        # pooled_output (=y1) is the output of the CLS token\n",
        "        # \"Since BERT is transformer based contextual model, the idea is [CLS] token would have captured the entire context and would be sufficient for simple downstream tasks such as classification.\"\n",
        "        # https://stackoverflow.com/questions/63673511/how-to-use-the-outputs-of-bert-model?rq=1\n",
        "        # https://towardsdatascience.com/bert-to-the-rescue-17671379687f\n",
        "        y1 = self.model1_layer3(y1)\n",
        "        \n",
        "        y2 = self.model2_layer1(x[2])\n",
        "\n",
        "        y = torch.cat([y1, y2], 1)\n",
        "        y = self.linear(y)\n",
        "        y = self.relu(y)\n",
        "        y = self.linear2(y)\n",
        "        y = self.linear3(y)\n",
        "        return self.sigmoid(y)"
      ],
      "metadata": {
        "id": "WPEV5kKOhI8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJdsCzCuqzHQ"
      },
      "outputs": [],
      "source": [
        "device = 'cuda'\n",
        "\n",
        "import numpy as np\n",
        "item_text_embeddings_size = 128\n",
        "user_embeddings_size = 100\n",
        "item_embeddings_size = 10#128\n",
        "\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME) \n",
        "dataset_train= LFM2bDataset(data_interaction_train,tokenizer,item_text_embeddings_size, text_col,  item2pos, user2id)\n",
        "dataset_val= LFM2bDataset(data_interaction_val,tokenizer,item_text_embeddings_size, text_col,  item2pos, user2id)\n",
        "\n",
        "# dataset_train= LFM2bDatasetMulitpleText(data_interaction_train,tokenizer,item_text_embeddings_size,  item2pos, user2id)\n",
        "# dataset_val= LFM2bDatasetMulitpleText(data_interaction_val,tokenizer,item_text_embeddings_size,  item2pos, user2id)\n",
        "\n",
        "num_users = len(data_interaction['user_id'].drop_duplicates())\n",
        "num_items = len(pos2item)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaONzUp8rsKH"
      },
      "source": [
        "# Interpretability of Gender Bias\n",
        "\n",
        "- Find correlations between user gender and item gender (Pearsons correlation)\n",
        "- For each track get proportion of female/male user => compare train and recommendataion data\n",
        "- Check if genderness increased/decreased in recommendations (in comparision to training data) \n",
        "- Compare distribution of genderness between history and recommendations \n",
        "  - \"Delta Metric of Genderness\" (https://arxiv.org/pdf/2108.06973.pdf)\n",
        "  - Proportion tests: Fisher exact test, Chi-Square\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get feature vectors"
      ],
      "metadata": {
        "id": "0oWwJyvwn2z9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_feature_vector(model, inputs, batch_size):\n",
        "\n",
        "  layer = model._modules.get('model1_layer3')\n",
        "\n",
        "  embedding = torch.zeros((batch_size, 128))\n",
        "  def copy_data(m, i, o):\n",
        "          embedding.copy_(o.data)\n",
        "  h = layer.register_forward_hook(copy_data)\n",
        "  model(inputs)\n",
        "  h.remove()\n",
        "  return embedding"
      ],
      "metadata": {
        "id": "9qoYc5VwgnWL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_vectors = pd.DataFrame()\n",
        "batch_size= 12\n",
        "dataset_test= LFM2bDataset(data_interaction_test,tokenizer,item_text_embeddings_size, text_col,  item2pos, user2id)\n",
        "#dataset_test= LFM2bDatasetMulitpleText(data_interaction_test,tokenizer,item_text_embeddings_size,  item2pos, user2id)\n",
        "dataloader_test=DataLoader(dataset=dataset_test,batch_size=batch_size, num_workers=4)\n",
        "\n",
        "# hidden_dense_layer_size_base = item_embeddings_size + user_embeddings_size\n",
        "# model = AMARBase(hidden_dense_layer_size_base, item_embeddings_size, user_embeddings_size, num_users, num_items)\n",
        "\n",
        "# hidden_dense_layer_size_bert = item_embeddings_size + 2 * item_text_embeddings_size + user_embeddings_size\n",
        "# model = AMARBertMultiple( hidden_dense_layer_size_bert, item_embeddings_size, user_embeddings_size, item_text_embeddings_size, num_users, num_items)\n",
        "\n",
        "hidden_dense_layer_size_bert = item_embeddings_size + item_text_embeddings_size + user_embeddings_size\n",
        "model = AMARBertEmb( hidden_dense_layer_size_bert, item_embeddings_size, user_embeddings_size, item_text_embeddings_size, num_users, num_items)\n",
        "\n",
        "# hidden_dense_layer_size_bert = user_embeddings_size  +item_text_embeddings_size \n",
        "# model = AMARBert(hidden_dense_layer_size_bert, user_embeddings_size, num_users)\n",
        "\n",
        "model.load_state_dict(torch.load(model_dir + 'best_model.pth'))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "\n",
        "for data in dataloader_test:\n",
        "  #items positions\n",
        "  curr_items_ids_batch = data['track_id']\n",
        "  # items descriptions\n",
        "  curr_items_batch = data['input_ids_lyrics']\n",
        "  curr_attentions_batch = data['attention_mask_lyrics']\n",
        "\n",
        "  # additional items descriptions\n",
        "  # curr_items_batch1 = data['input_ids_tags']\n",
        "  # curr_attentions_batch1 = data['attention_mask_tags']\n",
        "\n",
        "  # users ids\n",
        "  curr_users_batch =data['user_id']\n",
        "\n",
        "  # model inputs\n",
        "  inputs = [curr_items_ids_batch.type(torch.LongTensor).to(device), \n",
        "            curr_items_batch.type(torch.LongTensor).to(device), \n",
        "            curr_users_batch.type(torch.LongTensor).to(device), \n",
        "            curr_attentions_batch.type(torch.LongTensor).to(device),\n",
        "            # curr_items_batch1.type(torch.LongTensor).to(device), \n",
        "            # curr_attentions_batch1.type(torch.LongTensor).to(device),\n",
        "            ]\n",
        "\n",
        "\n",
        "  with torch.no_grad():\n",
        "    outputs = model(inputs)\n",
        "\n",
        "    \n",
        "  # save embedding layer for each item\n",
        "\n",
        "  # save prediction for each user\n",
        "  item_list=[]\n",
        "  for i in range(outputs.shape[0]):\n",
        "    item_list.append(pos2item[curr_items_ids_batch[i].item()])\n",
        "    \n",
        "  feature_vector = get_feature_vector(model, inputs, batch_size)\n",
        "  fv = pd.DataFrame(feature_vector)\n",
        "  fv.index = item_list\n",
        "  feature_vectors = feature_vectors.append(fv)\n",
        "  break\n",
        "      "
      ],
      "metadata": {
        "id": "jcBP8cIYn2Tq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_vectors"
      ],
      "metadata": {
        "id": "aMtoySmyiBw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Descriptive analysis"
      ],
      "metadata": {
        "id": "VJa7Z6Lvu4Ay"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfJvYiuFjQ54"
      },
      "source": [
        "## Distribution of gender"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ty8fFzv_jQiN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5599dd92-84d9-431b-9cea-a0e416fe8f7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In the history data, 23.48 % female items were consumed.\n",
            "In the test data, 23.62 % female items were consumed.\n",
            "In the recommendation data, 29.78 % female items were recommended.\n"
          ]
        }
      ],
      "source": [
        "print(f\"In the history data, {round(df_train.gender_artist.sum() / len(df_train) * 100, 2)} % female items were consumed.\")\n",
        "print(f\"In the test data, {round(df_test.gender_artist.sum() / len(df_test) * 100, 2)} % female items were consumed.\")\n",
        "print(f\"In the recommendation data, {round(df_rec.gender_artist.sum() / len(df_rec) * 100, 2)} % female items were recommended.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgDbgyUFYVPF"
      },
      "source": [
        "## Correlation between user and item gender"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4aOub4YUaga",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de34c93e-0a1b-4d8a-9920-26b200996267"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In the history data, all female users listen to 42.54 % female items.\n",
            "In the history data, all male users listen to 21.17 % female items.\n",
            "The attributes gender_user and gender_artist show a correlation of  0.11403506272269025\n"
          ]
        }
      ],
      "source": [
        "# Proportion of female items in all female user\n",
        "prop_female = 0\n",
        "# Proportion of female items in all male user\n",
        "prop_male = 0\n",
        "for group, df in df_test.groupby('gender_user'):\n",
        "  if group == 0:\n",
        "    prop_male = df.gender_artist.sum() / len(df)\n",
        "  else:\n",
        "    prop_female = df.gender_artist.sum() / len(df)\n",
        "\n",
        "print(f'In the history data, all female users listen to {round(prop_female*100, 2)} % female items.')\n",
        "print(f'In the history data, all male users listen to {round(prop_male*100, 2)} % female items.')\n",
        "\n",
        "# Phi correlation (http://web.pdx.edu/~newsomj/pa551/lectur15.htm)\n",
        "print('The attributes gender_user and gender_artist show a correlation of ', df_train.gender_user.corr(df_train.gender_artist))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0eFAQadph9Yz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6b803ae-d2bd-4cf0-d176-640bd022a206"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In the recommendation data, to all female users 30.43 % female items are recommended.\n",
            "In the recommendation data, to all male users 29.72 % female items are recommended.\n",
            "The attributes gender_user and gender_artist show a correlation of  0.004405760194930985\n"
          ]
        }
      ],
      "source": [
        "# Proportion of female items in all female user\n",
        "prop_female = 0\n",
        "# Proportion of female items in all male user\n",
        "prop_male = 0\n",
        "for group, df in df_rec.groupby('gender_user'):\n",
        "  if group == 0:\n",
        "    prop_male = df.gender_artist.sum() / len(df)\n",
        "  else:\n",
        "    prop_female = df.gender_artist.sum() / len(df)\n",
        "\n",
        "print(f'In the recommendation data, to all female users {round(prop_female*100, 2)} % female items are recommended.')\n",
        "print(f'In the recommendation data, to all male users {round(prop_male*100, 2)} % female items are recommended.')\n",
        "\n",
        "# Phi correlation\n",
        "print('The attributes gender_user and gender_artist show a correlation of ', df_rec.gender_user.corr(df_rec.gender_artist))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEiaFN1bYdFE"
      },
      "source": [
        "## Delta metric of genderness\n",
        "\n",
        " For user u_i: (prop_female(rec) - prop_female(history)) / prop_female(history)\n",
        "\n",
        " If positive: more female tracks are recommended to the user"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JP_j3yQgd6Gx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e407267-5893-4e09-ebd3-96027698ee6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The value of delta is 0.2685 and therefore more female tracks are recommended to user.\n"
          ]
        }
      ],
      "source": [
        "prop_female_rec = df_rec.gender_artist.sum() / len(df_rec)\n",
        "prop_female_history = df_train.gender_artist.sum() / len(df_train)\n",
        "delta = (prop_female_rec - prop_female_history) / prop_female_history\n",
        "\n",
        "if delta > 0:\n",
        "  print(f'The value of delta is {round(delta, 4)} and therefore more female tracks are recommended to user.')\n",
        "else:\n",
        "  print(f'The value of delta is {round(delta, 4)} and therefore more male tracks are recommended to user.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_W5yg61kZJOV"
      },
      "source": [
        "## Proportion distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IV5pBGTjYfsO"
      },
      "outputs": [],
      "source": [
        "prop_all = []\n",
        "prop_male = []\n",
        "prop_female = []\n",
        "for user, df in df_train.groupby('user_id'):\n",
        "  p = df.gender_artist.sum() / len(df)\n",
        "  prop_all.append(p)\n",
        "  if (df.gender_user).all() == 0:\n",
        "    prop_male.append(p)\n",
        "  else: prop_female.append(p)\n",
        "\n",
        "print(f\"In the history data, there were on average {round(np.mean(prop_all)*100, 2)} % female items consumed.\")\n",
        "print(f\"In the history data, among all male users there were on average {round(np.mean(prop_male)*100, 2)} % female items consumed.\")\n",
        "print(f\"In the history data, among all female users there were on average {round(np.mean(prop_female)*100, 2)} % female items consumed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikI_8GX1k2lI"
      },
      "outputs": [],
      "source": [
        "prop_all = []\n",
        "prop_male = []\n",
        "prop_female = []\n",
        "for user, df in df_rec.groupby('user_id'):\n",
        "  p = df.gender_artist.sum() / len(df)\n",
        "  prop_all.append(p)\n",
        "  if (df.gender_user).all() == 0:\n",
        "    prop_male.append(p)\n",
        "  else: prop_female.append(p)\n",
        "\n",
        "print(f\"In the recommendation data, there are on average {round(np.mean(prop_all)*100, 2)} % female items recommended.\")\n",
        "print(f\"In the recommendation data, among all male users there are on average {round(np.mean(prop_male)*100, 2)} % female items recommended.\")\n",
        "print(f\"In the recommendation data, among all female users there are on average {round(np.mean(prop_female)*100, 2)} % female items recommended.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjjmSaKW-99W"
      },
      "source": [
        "# BERT classification model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kF1vpMAVGwwQ"
      },
      "outputs": [],
      "source": [
        "%cd /content/drive/MyDrive/Colab\\ Notebooks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zj90aXI0_BYc"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "from smooth_gradient import SmoothGradient\n",
        "from integrated_gradient import IntegratedGradient\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import DistilBertConfig, DistilBertTokenizer\n",
        "from transformers import BertTokenizer, BertModel, AdamW\n",
        "\n",
        "from IPython.display import display, HTML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SR_66w52_s_4"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "class ArtistDataset(Dataset):\n",
        "    def __init__(self, data_all,tokenizer,max_length, text_col):\n",
        "        super(ArtistDataset, self).__init__()\n",
        "        self.data_all = data_all\n",
        "        self.max_length = max_length\n",
        "        self.tokenizer = tokenizer\n",
        "        self.text_col = text_col\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.data_all)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        \n",
        "        text1 = self.data_all.iloc[index][self.text_col]\n",
        "        \n",
        "        inputs = tokenizer.encode_plus(\n",
        "            text1, \n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length = self.max_length,\n",
        "            pad_to_max_length=True\n",
        "            \n",
        "        )\n",
        "        ids = inputs[\"input_ids\"]\n",
        "        token_type_ids = inputs[\"token_type_ids\"]\n",
        "        mask = inputs[\"attention_mask\"]\n",
        "\n",
        "        return {\n",
        "            'input_ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'attention_mask': torch.tensor(mask, dtype=torch.long),\n",
        "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
        "            'target': torch.tensor(self.data_all.iloc[index]['gender_artist'], dtype=torch.float)\n",
        "            }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJyXCdXwSd_D"
      },
      "outputs": [],
      "source": [
        "class SentimentClassifier(nn.Module):\n",
        "  def __init__(self, n_classes=2):\n",
        "    super(SentimentClassifier, self).__init__()\n",
        "    self.bert = BertModel.from_pretrained('prajjwal1/bert-tiny')\n",
        "    self.drop = nn.Dropout(p=0.3)\n",
        "    self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
        "\n",
        "  def forward(self, input_ids, attention_mask):\n",
        "    y, pooled_output = self.bert(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask, return_dict=False\n",
        "    )\n",
        "    pooled_output = y[:,0,:]\n",
        "    output = self.drop(pooled_output)\n",
        "    return self.out(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWIGzeWVSpea"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.nn.functional import softmax\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "class SaliencyInterpreter:\n",
        "    def __init__(self,\n",
        "                 model,\n",
        "                 criterion,\n",
        "                 tokenizer,\n",
        "                 show_progress=True,\n",
        "                 **kwargs):\n",
        "\n",
        "        \"\"\"\n",
        "        :param model: nn.Module object - can be HuggingFace's model or custom one.\n",
        "        :param criterion: torch criterion used to train your model.\n",
        "        :param tokenizer: HuggingFace's tokenizer.\n",
        "        :param show_progress: bool flag to show tqdm progress bar.\n",
        "        :param kwargs:\n",
        "            encoder: string indicates the HuggingFace's encoder, that has 'embeddings' attribute. Used\n",
        "                if your model doesn't have 'get_input_embeddings' method to get access to encoder embeddings\n",
        "        \"\"\"\n",
        "\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.model = model.to(self.device)\n",
        "        self.model.eval()\n",
        "        self.criterion = criterion\n",
        "        self.tokenizer = tokenizer\n",
        "        self.show_progress = show_progress\n",
        "        self.kwargs = kwargs\n",
        "        # to save outputs in saliency_interpret\n",
        "        self.batch_output = None\n",
        "\n",
        "    def _get_gradients(self, batch):\n",
        "        # set requires_grad to true for all parameters, but save original values to\n",
        "        # restore them later\n",
        "        original_param_name_to_requires_grad_dict = {}\n",
        "        for param_name, param in self.model.named_parameters():\n",
        "            original_param_name_to_requires_grad_dict[param_name] = param.requires_grad\n",
        "            param.requires_grad = True\n",
        "        embedding_gradients = []\n",
        "        hooks = self._register_embedding_gradient_hooks(embedding_gradients)\n",
        "\n",
        "        loss = self.forward_step(batch)\n",
        "\n",
        "        self.model.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        for hook in hooks:\n",
        "            hook.remove()\n",
        "\n",
        "        # restore the original requires_grad values of the parameters\n",
        "        for param_name, param in self.model.named_parameters():\n",
        "            param.requires_grad = original_param_name_to_requires_grad_dict[param_name]\n",
        "\n",
        "        return embedding_gradients[0]\n",
        "\n",
        "    def _register_embedding_gradient_hooks(self, embedding_gradients):\n",
        "        \"\"\"\n",
        "        Registers a backward hook on the\n",
        "        Used to save the gradients of the embeddings for use in get_gradients()\n",
        "        When there are multiple inputs (e.g., a passage and question), the hook\n",
        "        will be called multiple times. We append all the embeddings gradients\n",
        "        to a list.\n",
        "        \"\"\"\n",
        "\n",
        "        def hook_layers(module, grad_in, grad_out):\n",
        "            embedding_gradients.append(grad_out[0])\n",
        "\n",
        "        backward_hooks = []\n",
        "        embedding_layer = self.get_embeddings_layer()\n",
        "        backward_hooks.append(embedding_layer.register_backward_hook(hook_layers))\n",
        "        return backward_hooks\n",
        "\n",
        "    def get_embeddings_layer(self):\n",
        "        if hasattr(self.model, \"get_input_embeddings\"):\n",
        "            embedding_layer = self.model.get_input_embeddings()\n",
        "        else:\n",
        "            encoder_attribute = self.kwargs.get(\"encoder\")\n",
        "            assert encoder_attribute, \"Your model doesn't have 'get_input_embeddings' method, thus you \" \\\n",
        "                \"have provide 'encoder' key argument while initializing SaliencyInterpreter object\"\n",
        "            embedding_layer = getattr(self.model, encoder_attribute).embeddings\n",
        "        return embedding_layer\n",
        "\n",
        "    def colorize(self, instance, skip_special_tokens=False):\n",
        "\n",
        "        special_tokens = self.special_tokens\n",
        "\n",
        "        word_cmap = matplotlib.cm.Blues\n",
        "        prob_cmap = matplotlib.cm.Greens\n",
        "        template = '<span class=\"barcode\"; style=\"color: black; background-color: {}\">{}</span>'\n",
        "        colored_string = ''\n",
        "        # Use a matplotlib normalizer in order to make clearer the difference between values\n",
        "        normalized_and_mapped = matplotlib.cm.ScalarMappable(cmap=word_cmap).to_rgba(instance['grad'])\n",
        "        for word, color in zip(instance['tokens'], normalized_and_mapped):\n",
        "            if word in special_tokens and skip_special_tokens:\n",
        "                continue\n",
        "            # handle wordpieces\n",
        "            word = word.replace(\"##\", \"\") if \"##\" in word else ' ' + word\n",
        "            color = matplotlib.colors.rgb2hex(color[:3])\n",
        "            colored_string += template.format(color, word)\n",
        "        colored_string += template.format(0, \"    Label: {} |\".format(instance['label']))\n",
        "        prob = instance['prob']\n",
        "        color = matplotlib.colors.rgb2hex(prob_cmap(prob)[:3])\n",
        "        colored_string += template.format(color, \"{:.2f}%\".format(instance['prob']*100)) + '|'\n",
        "        return colored_string\n",
        "\n",
        "    @property\n",
        "    def special_tokens(self):\n",
        "        \"\"\"\n",
        "        Some tokenizers don't have 'eos_token' and 'bos_token' attributes.\n",
        "        So needed we some trick to get them.\n",
        "        \"\"\"\n",
        "        if self.tokenizer.bos_token is None or self.tokenizer.eos_token is None:\n",
        "            special_tokens = self.tokenizer.build_inputs_with_special_tokens([])\n",
        "            special_tokens_ids = self.tokenizer.convert_ids_to_tokens(special_tokens)\n",
        "            self.tokenizer.bos_token, self.tokenizer.eos_token = special_tokens_ids\n",
        "\n",
        "        special_tokens = self.tokenizer.eos_token, self.tokenizer.bos_token\n",
        "        return special_tokens\n",
        "\n",
        "    def forward_step(self, batch):\n",
        "        \"\"\"\n",
        "        If your model receive inputs in another way or you computing not\n",
        "         like in this example simply override this method. It should return the batch loss\n",
        "        :param batch: batch returned by dataloader\n",
        "        :return: torch.Tensor: batch loss\n",
        "        \"\"\"\n",
        "        input_ids = batch.get('input_ids').to(self.device)\n",
        "        attention_mask = batch.get(\"attention_mask\").to(self.device)\n",
        "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        label = torch.argmax(outputs, dim=1)\n",
        "        batch_losses = self.criterion(outputs, label)\n",
        "        loss = torch.mean(batch_losses)\n",
        "\n",
        "        self.batch_output = [input_ids, outputs]\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def update_output(self):\n",
        "        \"\"\"\n",
        "        You can also override this method if you want to change the format\n",
        "         of outputs. (e.g. store just gradients)\n",
        "        :return: batch_output\n",
        "        \"\"\"\n",
        "\n",
        "        input_ids, outputs, grads = self.batch_output\n",
        "\n",
        "        probs = softmax(outputs, dim=-1)\n",
        "        probs, labels = torch.max(probs, dim=-1)\n",
        "\n",
        "        tokens = [\n",
        "            self.tokenizer.convert_ids_to_tokens(input_ids_)\n",
        "            for input_ids_ in input_ids\n",
        "        ]\n",
        "\n",
        "        embedding_grads = grads.sum(dim=2)\n",
        "        # norm for each sequence\n",
        "        norms = torch.norm(embedding_grads, dim=1, p=1)\n",
        "        # normalizing\n",
        "        for i, norm in enumerate(norms):\n",
        "            embedding_grads[i] = torch.abs(embedding_grads[i]) / norm\n",
        "\n",
        "        batch_output = []\n",
        "\n",
        "        iterator = zip(tokens, probs, embedding_grads, labels)\n",
        "\n",
        "        for example_tokens, example_prob, example_grad, example_label in iterator:\n",
        "            example_dict = dict()\n",
        "            # as we do it by batches we has a padding so we need to remove it\n",
        "            example_tokens = [t for t in example_tokens if t != self.tokenizer.pad_token]\n",
        "            example_dict['tokens'] = example_tokens\n",
        "            example_dict['grad'] = example_grad.cpu().tolist()[:len(example_tokens)]\n",
        "            example_dict['label'] = example_label.item()\n",
        "            example_dict['prob'] = example_prob.item()\n",
        "            batch_output.append(example_dict)\n",
        "        return batch_output\n",
        "\n",
        "import torch\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "\n",
        "class SmoothGradient(SaliencyInterpreter):\n",
        "    \"\"\"\n",
        "    Interprets the prediction using SmoothGrad (https://arxiv.org/abs/1706.03825)\n",
        "    Registered as a `SaliencyInterpreter` with name \"smooth-gradient\".\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 model,\n",
        "                 criterion,\n",
        "                 tokenizer,\n",
        "                 stdev=0.01,\n",
        "                 num_samples=20,\n",
        "                 show_progress=True,\n",
        "                 **kwargs):\n",
        "        super().__init__(model, criterion, tokenizer, show_progress, **kwargs)\n",
        "        # Hyperparameters\n",
        "        self.stdev = stdev\n",
        "        self.num_samples = num_samples\n",
        "\n",
        "    def saliency_interpret(self, test_dataloader):\n",
        "\n",
        "        instances_with_grads = []\n",
        "        iterator = tqdm(test_dataloader) if self.show_progress else test_dataloader\n",
        "\n",
        "        for batch in iterator:\n",
        "\n",
        "            # we will store there batch outputs such as gradients, probability, tokens\n",
        "            # so as each of them are used in different places, for convenience we will create\n",
        "            # it as attribute:\n",
        "            self.batch_output = []\n",
        "            self._smooth_grads(batch)\n",
        "            batch_output = self.update_output()\n",
        "            instances_with_grads.extend(batch_output)\n",
        "\n",
        "        return instances_with_grads\n",
        "\n",
        "    def _register_forward_hook(self, stdev: float):\n",
        "        \"\"\"\n",
        "        Register a forward hook on the embedding layer which adds random noise to every embedding.\n",
        "        Used for one term in the SmoothGrad sum.\n",
        "        \"\"\"\n",
        "\n",
        "        def forward_hook(module, inputs, output):\n",
        "            # Random noise = N(0, stdev * (max-min))\n",
        "            scale = output.detach().max() - output.detach().min()\n",
        "            noise = torch.randn(output.shape).to(output.device) * stdev * scale\n",
        "\n",
        "            # Add the random noise\n",
        "            output.add_(noise)\n",
        "\n",
        "        # Register the hook\n",
        "        embedding_layer = self.get_embeddings_layer()\n",
        "        handle = embedding_layer.register_forward_hook(forward_hook)\n",
        "        return handle\n",
        "\n",
        "    def _smooth_grads(self, batch):\n",
        "        total_gradients = None\n",
        "        for _ in range(self.num_samples):\n",
        "            handle = self._register_forward_hook(self.stdev)\n",
        "            grads = self._get_gradients(batch)\n",
        "            handle.remove()\n",
        "\n",
        "            # Sum gradients\n",
        "            if total_gradients is None:\n",
        "                total_gradients = grads\n",
        "            else:\n",
        "                total_gradients = total_gradients + grads\n",
        "\n",
        "        total_gradients /= self.num_samples\n",
        "\n",
        "        self.batch_output.append(total_gradients)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fKATY80EQhcn"
      },
      "outputs": [],
      "source": [
        "data_classification = data_classification.drop_duplicates()\n",
        "\n",
        "class_0 = data_classification[data_classification['gender_artist'] == 0]\n",
        "class_1 = data_classification[data_classification['gender_artist'] == 1]\n",
        "class_count_0, class_count_1 = data_classification['gender_artist'].value_counts()\n",
        "\n",
        "class_0_under = class_0.sample(class_count_1)\n",
        "test_under = pd.concat([class_0_under, class_1], axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FmkX_fFg_vyD"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') \n",
        "\n",
        "dataset_train= ArtistDataset(test_under,tokenizer, 128, 'lyrics_cleaned')\n",
        "\n",
        "dataloader_train=DataLoader(dataset=dataset_train,batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NbhJiB2DcyPg"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda'"
      ],
      "metadata": {
        "id": "wgnJxoT5n77x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-O0hGQ33_yCm"
      },
      "outputs": [],
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "# model = BertForSequenceClassification.from_pretrained(\n",
        "#     'bert-base-uncased', \n",
        "#     num_labels = 2, \n",
        "#     output_attentions = False, \n",
        "#     output_hidden_states = False, \n",
        "# ).to(device)\n",
        "model = SentimentClassifier().to('cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plJ9-b0g_0Rn"
      },
      "outputs": [],
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
        "epochs = 20\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)\n",
        "total_steps = len(dataloader_train) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, \n",
        "                                            num_training_steps = total_steps)\n",
        "\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    losses = []\n",
        "    model.train()\n",
        "\n",
        "    for step, batch in enumerate(dataloader_train):\n",
        "\n",
        "        b_input_ids = batch['input_ids'].to(device)\n",
        "        b_input_mask = batch['attention_mask'].to(device)\n",
        "        b_labels = batch['target'].type(torch.LongTensor).to(device)\n",
        "    \n",
        "\n",
        "        outputs = model(\n",
        "          input_ids=b_input_ids,\n",
        "          attention_mask=b_input_mask\n",
        "        )#['logits']\n",
        "\n",
        "        _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "        loss = loss_fn(outputs, b_labels)\n",
        "\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "        \n",
        "        scheduler.step()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "    print(f\"Epoch {epoch_i}, Loss {np.mean(losses)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs"
      ],
      "metadata": {
        "id": "acrNzDUDoqEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0Nd2qnP_5lv"
      },
      "outputs": [],
      "source": [
        "integrated_grad = SmoothGradient(\n",
        "    model, \n",
        "    loss_fn, \n",
        "    tokenizer, \n",
        "    show_progress=False,\n",
        "    encoder=\"bert\"\n",
        ")\n",
        "instances = integrated_grad.saliency_interpret(dataloader_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sacNXcd0KEpV"
      },
      "outputs": [],
      "source": [
        "words_male = []\n",
        "words_female = []\n",
        "for i in range(len(instances)):\n",
        "  idx = np.argsort(instances[i]['grad'])[::-1][:10]\n",
        "  if instances[i]['label'] == 0:\n",
        "    words_male = words_male + [instances[i]['tokens'][val] for val in idx]\n",
        "  else: \n",
        "    words_female = words_female + [instances[i]['tokens'][val] for val in idx]\n",
        "\n",
        "words_male = [ elem for elem in words_male if elem not in ['[SEP]', '[CLS]']]\n",
        "words_female = [ elem for elem in words_female if elem not in ['[SEP]', '[CLS]']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-iwKmY9dyT0"
      },
      "outputs": [],
      "source": [
        "len(words_male)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3CPgQBRS_7ms"
      },
      "outputs": [],
      "source": [
        "coloder_string = integrated_grad.colorize(instances[1])\n",
        "display(HTML(coloder_string))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YnwG--ONLiLM"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "#convert list to string and generate\n",
        "unique_string=(\" \").join(words_female)\n",
        "wordcloud = WordCloud(width = 1000, height = 500).generate(unique_string)\n",
        "plt.figure(figsize=(15,8))\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "print(wordcloud.words_.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hFWzsRuNXLYd"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "#convert list to string and generate\n",
        "unique_string=(\" \").join(words_male)\n",
        "wordcloud = WordCloud(width = 1000, height = 500).generate(unique_string)\n",
        "plt.figure(figsize=(15,8))\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "print(wordcloud.words_.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtIQzQEH2vhy"
      },
      "source": [
        "# Predict artist gender"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_NzF7QQ3J6Z"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn import metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FducXCFxKURy"
      },
      "outputs": [],
      "source": [
        "data_all_info = pd.read_csv(\"music/data_all_info.txt\", sep=\"\\t\").drop(['Unnamed: 0'],axis=1).dropna()\n",
        "data_all_info = data_all_info.loc[data_all_info.artist_gender != 'other' ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8XV1iDFGB0mF"
      },
      "outputs": [],
      "source": [
        "input = feature_vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zv1QvFM3I22n"
      },
      "outputs": [],
      "source": [
        "df_features = pd.DataFrame()\n",
        "id_col = []\n",
        "\n",
        "for id in data_interaction_test[item_col].unique():\n",
        "  id_col.append(id)\n",
        "  df_features = df_features.append(pd.DataFrame(input[id][0].detach().numpy()))\n",
        "\n",
        "df_features[item_col] = id_col"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EBOE0G_TJ28n"
      },
      "outputs": [],
      "source": [
        "df_features = pd.merge(df_features, data_all_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOQ6VKT_IVyg"
      },
      "outputs": [],
      "source": [
        "df_features.loc[df_features.artist_gender == 'female', 'artist_gender'] = 0\n",
        "df_features.loc[df_features.artist_gender == 'male', 'artist_gender'] = 1\n",
        "df_features.artist_gender = df_features.artist_gender.astype('int')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ic2xa382unN"
      },
      "outputs": [],
      "source": [
        "X = df_features.iloc[:,:128]\n",
        "y = df_features.artist_gender\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.4, random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h50TBcLs3G7l"
      },
      "outputs": [],
      "source": [
        "#clf = RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1)\n",
        "clf = DecisionTreeClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred=clf.predict(X_test)\n",
        "\n",
        "print(\"Accuracy:\", metrics.accuracy_score(y_test.values, y_pred))\n",
        "print(\"Balanced Accuracy:\", metrics.balanced_accuracy_score(y_test.values, y_pred))\n",
        "print(\"Recall: \", metrics.recall_score(y_test.values, y_pred))\n",
        "print(\"Precision: \", metrics.precision_score(y_test.values, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQSgoDfe4Cc4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "feature_imp = pd.Series(clf.feature_importances_,index=X.columns).sort_values(ascending=False)\n",
        "feature_imp.head(10)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "0c67LS6n1Y-A",
        "K-A_VQKP1hFS",
        "jc6rPo1eHeca",
        "hgDbgyUFYVPF",
        "UEiaFN1bYdFE",
        "_W5yg61kZJOV",
        "xjjmSaKW-99W",
        "NtIQzQEH2vhy"
      ],
      "machine_shape": "hm",
      "name": "004_interpretation.ipynb",
      "provenance": [],
      "mount_file_id": "1oLwqD4kIwkhfA4JSXv2yb_2VYFXGEB35",
      "authorship_tag": "ABX9TyPsIS2hKFACLfnnjEc2cicM",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
